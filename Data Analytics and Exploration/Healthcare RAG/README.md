# Watt AI Hospital Reports Project RAG Framework

This repository is intended to serve as a retrieval-augmented generation (RAG) framework, which retrieves relevant documents from a given data source to be used to prompt a large language model (LLM).

## Getting Started

Due to the resource-intensive nature of text generation using an LLM, it is necessary to have adequate compute power to run the scripts in this repository. As such, it is recommended to clone this repository onto the Palmetto Cluster or any other compute cluster that can provide such resources.

After cloning the repository onto a machine with adequate resources, the following scripts can be run:
1. **get_embeddings.py** - Run this script with the path to a dataset as an argument to get the vector embeddings from the data needed to find the most relevant data for the relevance query. The script should be run in the following format: "python3 get_embeddings.py <dataset_path_here>". The script currently only supports datasets in CSV or JSON format. After the script is run, you will be prompted to enter the name of the column in the dataset that contains the page content. This means that you must select the column (or key) of data in your dataset that is relevant to the task that you are trying to complete with the RAG framework.
2. **create_prompt.py**  - After running get_embeddings.py, run this script to create a prompt based on the generated vector embeddings that will be fed into the LLM. You can choose to optionally provide an argument for the path to a dataset to use a specific column of the data as the relevance query. Only datasets in CSV or JSON format are currently supported. If an argument is not provided, you will be prompted to enter the relevance query yourself as text in the command line. You will also be prompted to enter the number of relevant examples you want in your prompt as determined by similarity scores between the embedded data and the relevance query. The script can be run in the following format: "python3 create_prompt.py <optional_dataset_path_here>".
3. **query_for_results.py** - Run this script to query an LLM using the prompt generated by the previous script (the prompt is read from the "rag_prompt.txt" file that is generated which can be freely edited). It can be run by the following command: "python3 query_for_results.py".

### Prerequisites

To install the necessary Python packages needed to run the scripts, run the following command (preferably within a virtual environment or conda environment): "pip install -r requirements.txt".

If you plan to use Llama 2 to generate your responses (the default LLM model for the query_for_results.py script), you must have been granted access to Llama 2 by Meta. Once you have been granted access and have access in your Hugging Face account, you must log in to your account in the command line with an access token linked to your account. See the Hugging Face documentation for more detailed instructions on this process.

### Palmetto Cluster Instructions

To run the scripts in this repository on the Palmetto Cluster, follow these steps:
1. Start an interactive job through the Palmetto shell by running the following command (the resources can be changed according to need): "qsub -I -l select=1:ncpus=16:ngpus=1:mem=64gb:gpu_model=a100 -l walltime=01:00:00".
2. To utilize GPUs, add the CUDA module by running the following command: "module add cuda/12.1.1-gcc/9.5.0".
3. If using a conda environment (recommended), add the conda module by running the following command: "module add anaconda3/2022.05-gcc/9.5.0".
