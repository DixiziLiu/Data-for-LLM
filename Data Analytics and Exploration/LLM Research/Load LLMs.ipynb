{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d88ed6-f722-4b27-97dc-ad8dff669fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, using /scratch/dixizil/hf_cache for huggingface cache. Models will be stored there.\n",
      "Huggingface API key loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set cache directory and load Huggingface api key\n",
    "\n",
    "import os\n",
    "\n",
    "username = os.getenv('USER')\n",
    "directory_path = os.path.join('/scratch',username)\n",
    "\n",
    "# Set Huggingface cache directory to be on scratch drive\n",
    "if os.path.exists(directory_path):\n",
    "    hf_cache_dir = os.path.join(directory_path,'hf_cache')\n",
    "    if not os.path.exists(hf_cache_dir):\n",
    "        os.mkdir(hf_cache_dir)\n",
    "    print(f\"Okay, using {hf_cache_dir} for huggingface cache. Models will be stored there.\")\n",
    "    assert os.path.exists(hf_cache_dir)\n",
    "    os.environ['TRANSFORMERS_CACHE'] = f'/scratch/{username}/hf_cache/'\n",
    "else:\n",
    "    error_message = f\"Are you sure you entered your username correctly? I couldn't find a directory {directory_path}.\"\n",
    "    raise FileNotFoundError(error_message)\n",
    "\n",
    "# Load Huggingface api key\n",
    "api_key_loc = os.path.join('/home', username, '.apikeys', 'huggingface_api_key.txt')\n",
    "\n",
    "if os.path.exists(api_key_loc):\n",
    "    print('Huggingface API key loaded.')\n",
    "else:\n",
    "    error_message = f'Huggingface API key not found. You need to get an HF API key from the HF website and store it at {api_key_loc}.\\n' \\\n",
    "                    'The API key will let you download models from Huggingface.'\n",
    "    raise FileNotFoundError(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61f8e33-7c0f-4948-a5a3-cb7a3c3ac102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38bf49-6959-4e8d-b8ce-f5b11f0f7d10",
   "metadata": {},
   "source": [
    "## GPT4All 13b Snoozy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fe1ce7-1bd3-4690-8cb2-3a516cee6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a70806053104ba69e14e481dfa46b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5257cb77ea4779aec8a6218d3cd9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfdb76045bd44a4bfa768f4896eabd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685b0a12ff7a4d71ac914a61218978e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c41bbf1ab5c47498f81f75aeb7ffac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f36f41289e44f4a876e3589b1e3138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb36f18783264814af3e413a0f9561f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00006.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18cd95cf0ce44ef9b6e30bc70588f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00006.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbe80c2449341769e301f7c7f7b0a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00006.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fe63f65b91467199021c217c22f9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00006.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf04acef7fa64737b9ef6b08277b4211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00006.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa72fb224914143a0ef8bb1a38145ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00006.bin:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1;/usr/sbin'), PosixPath('1;/opt/puppetlabs/bin'), PosixPath('1;/bin'), PosixPath('1;/opt/dell/srvadmin/bin'), PosixPath('1;/home/dixizil/bin'), PosixPath('1;/usr/bin'), PosixPath('1;/usr/bin/X11'), PosixPath('1;/usr/local/sbin'), PosixPath('1;/opt/pbs/default/sbin'), PosixPath('1'), PosixPath('1;/home/dixizil/.local/bin'), PosixPath('1;/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/bin'), PosixPath('1;/opt/pbs/default/bin')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('America/New_York')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ood/ondemand/root/usr/share/gems/3.0/bin'), PosixPath('/var/www/ood/apps/sys/dashboard/tmp/node_modules/yarn/bin'), PosixPath('/opt/ood/ondemand/root/usr/bin'), PosixPath('/opt/ood/ondemand/root/usr/sbin')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1;/software/ModuleFiles/modules/linux-rocky8-x86_64'), PosixPath('1;/usr/share/modulefiles'), PosixPath('1'), PosixPath('1;/software/AltModFiles')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('anaconda3/2022.05-gcc/9.5.0')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('1;/opt/puppetlabs/puppet/share/man'), PosixPath('1;/opt/pbs/default/share/man'), PosixPath('1;/opt/pbs/default/man'), PosixPath('1;/software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/man'), PosixPath('1'), PosixPath('1;/usr/share/lmod/lmod/share/man')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/pbs/default/man')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-8}\\n}'), PosixPath(\"() {  tr -cd 'a-zA-Z0-9' < /dev/urandom 2> /dev/null | head -c${1\")}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n}'), PosixPath('() {  local port=\"${1}\";\\n local time=\"${2')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n };\\n export -f wait_until_port_used;\\n function create_passwd () \\n { \\n tr -cd \\'a-zA-Z0-9\\' < /dev/urandom 2> /dev/null | head -c${1'), PosixPath('\"$2\" > /dev/null 2>&1\\n };\\n function port_used_bash () \\n { \\n local bash_supported=$(strings /bin/bash 2>/dev/null | grep tcp);\\n if [ \"$bash_supported\" == \"/dev/tcp/*/*\" ]; then\\n ( '), PosixPath('-8}\\n };\\n export -f create_passwd\\n}'), PosixPath(\" '\\\\(.*\\\\)\"), PosixPath('${port}\"; do\\n port=$(random_number \"${2'), PosixPath('-localhost}\";\\n local port=$(random_number \"${2'), PosixPath('-65535}\");\\n done;\\n echo \"${port}\"\\n };\\n export -f find_port;\\n function wait_until_port_used () \\n { \\n local port=\"${1}\";\\n local time=\"${2'), PosixPath('\\' || echo \"localhost\") | awk \\'END{print $NF}\\');\\n local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\\n for strategy in ${port_strategies[@]};\\n do\\n $strategy $host $port;\\n status=$?;\\n if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\\n return $status;\\n fi;\\n done;\\n return 127\\n };\\n export -f port_used;\\n function find_port () \\n { \\n local host=\"${1'), PosixPath('-2000}\" \"${3'), PosixPath('}\";\\n local host=$((expr \"${1}\" '), PosixPath(' < /dev/tcp/$1/$2 ) > /dev/null 2>&1;\\n else\\n return 127;\\n fi\\n };\\n function port_used () \\n { \\n local port=\"${1#*'), PosixPath('() {  function random_number () \\n { \\n shuf -i ${1}-${2} -n 1\\n };\\n export -f random_number;\\n function port_used_python () \\n { \\n python -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_python3 () \\n { \\n python3 -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_nc () \\n { \\n nc -w 2 \"$1\" \"$2\" < /dev/null > /dev/null 2>&1\\n };\\n function port_used_lsof () \\n { \\n lsof -i '), PosixPath('-65535}\");\\n while port_used \"${host}')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\\n}')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7787e9688b4bd59cb76668adf1e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da02693d8ba4bfa920bcda9b631b9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "model_id = 'nomic-ai/gpt4all-13b-snoozy'\n",
    "print('Loading tokenizer')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print('Loading model')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto')\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e071c8a-2a86-43c3-bc73-8878c0826756",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who will win the bundesliga this year?\"\n",
    "\n",
    "print(prompt + local_llm(prompt))\n",
    "\n",
    "#prompt = create_fsl_prompt(labeled_pirs, unlabeled_pir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a5b010-6003-4b1b-93a0-013e11bf2574",
   "metadata": {},
   "source": [
    "## Vicuna 13B v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756bf0a-4bfb-4819-989a-242360d972d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires adjusted weights\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-13b-delta-v1.1\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-13b-delta-v1.1\")\n",
    "\n",
    "print('Loading model')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-13b-delta-v1.1\", load_in_8bit=True, device_map='auto')\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298bcbb-03bb-46d4-9ca4-610cbc9837a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Who will win the bundesliga this year?\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e75c92-cde3-4424-a542-8b89748e2299",
   "metadata": {},
   "source": [
    "## Alpaca 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e58c3a5-d4f3-4c26-8319-fc0f8586dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260dce049bca40239da41089d281a5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a22b0bfd09f4f38bd81386a7bc6cdc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c12c0d80ed42389a1a06a6d7bdcca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be7843d05a74f1598bb110b9afd315d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a857bd555a1d42009d2c1baf8b12f946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a21cb974d4a6f930de0e359057ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c02811e11b94e5381aa3eb71dbef6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bf946648c847ba86be5de4e4d4eae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "model_id = 'chavinlo/alpaca-native'\n",
    "print('Loading tokenizer')\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print('Loading model')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             load_in_8bit=False, \n",
    "                                             # device_map='auto', \n",
    "                                             trust_remote_code=True)\n",
    "model.to('cpu')\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c12271-f005-44c5-820f-dbeb0fa4c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dixizil/.conda/envs/llms_env/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count to ten 1 2 3 4 5 6 7 8 9 10 (Count in a different language) Uno (Spanish): Uno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez. Two Languages: English and Spanish Together: Uno, dos, tres, four, five, six, seven, eight, nine, ten English-Language Instructions for the Player: • The player draws one card from the deck of cards • He or she looks at the card and decides if they would like to “keep” it or “pass” it to the next player • If they choose to keep the card, they put it face up on the playing surface • Players can also discard any other card from their hand when they pass a card • Once all players have passed their turn, the next player begins their turn by drawing a card from the remaining cards in the deck • The game is over when a player has only two cards left in his or her hands or the deck of cards is emptied out\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Count to ten 1 2 3 \"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a724352c-4cda-40a0-aded-955ca5986a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word length: 801\n",
      "token length: 951\n"
     ]
    }
   ],
   "source": [
    "text='Any minute now! Any minute now I will start listing the names of characters from Star Wars!' * 50\n",
    "tok_text = tokenizer(text)\n",
    "text_word_len = len(text.split(' '))\n",
    "text_tok_len = len(tok_text.input_ids)\n",
    "print(f'Word length: {text_word_len}\\ntoken length: {text_tok_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09072dc8-794c-4dfa-a59f-ab4cc8ef166b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now I will start listing the names of characters from Star Wars!Any minute now! Any minute now Isummary:\\nBy Markus on April 13th, 2019 at 4:16 pm'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_llm(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cc5cf42-9ffa-4752-a0dd-4f167838fadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25186285-c74f-439b-a4b9-ff94f40deb6a",
   "metadata": {},
   "source": [
    "## Mosaic mpt 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d361e0-aae5-4580-82ae-207c817b56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mosaicml/mpt-7b'\n",
    "\n",
    "print('Loading tokenizer')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print('Loading model')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', trust_remote_code=True)\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=.45,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4d0b8-eddd-4edb-82d4-6dfa1c86a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Count to ten 1 2 3\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f293d09-fb8a-4296-9bd8-37662f619407",
   "metadata": {},
   "source": [
    "## Mosaic mpt 7B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cc437-f6b5-4669-8f4a-459932a2a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mosaicml/mpt-7b-instruct'\n",
    "print('Loading tokenizer')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print('Loading model')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', trust_remote_code=True)\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=.45,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93662abc-95a9-42ad-9c2c-aaf61bb0c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Count to ten 1 2 3\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3278b4-e1b1-41fe-9268-fdea7029aa19",
   "metadata": {},
   "source": [
    "## OASST-Llama-30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc8d83-b5d8-47a5-be79-5302b78e7c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22ccdc5-9fab-4d2e-aa56-f55320adf553",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Alpaca 30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd51b6-a1a2-4179-8e73-d6b827f04153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VicUnlocked-alpaca-30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c543c6b0-bce8-4cf0-883c-81b1a3957710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cdbabd608948fa9b3d793e9d98c84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf4a3601357453085fea0f45b160c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02199a22962a47cf8aefc1ca153602f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152ce1c30e4041ecb58563f112d7ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebb86b8d3b34ee7a59cd9ee301a5c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b78a2e570145978cf3c561b8dfe365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/5.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae34ef06025f406da47ba656f84cce3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2703491b8a4d858ed9a350e0cbe364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Aeala/VicUnlocked-alpaca-30b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Aeala/VicUnlocked-alpaca-30b\")\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12911d90-0a7b-46fe-bc90-4a682d6453b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who will win the bundesliga this year? Bayern is still favorite but there are other teams that can surprise and make it interesting. The Bundesliga will be thrilling as usual, and we just cannot wait for August 2019 to come by.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who will win the bundesliga this year?\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516aeac-6795-4d0f-b8a9-303fbfab912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4-x-AlpacaDente2-30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209a05a-08ce-4e5a-87fc-ec6aa2801595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee0ebc8e6be42c08e2d5ae4d25d9c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Aeala/GPT4-x-AlpacaDente2-30b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Aeala/GPT4-x-AlpacaDente2-30b\")\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba0e379-5181-4396-9e4a-6d8522a4160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who will win the bundesliga this year?\n",
      "Bayern Munich is a strong favorite to retain their Bundesliga title, with Dortmund also vying for the top spot. However, it's notoriously difficult to predict football outcomes so anything could happen!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who will win the bundesliga this year?\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a57be0-d6d1-49a3-8979-711eae3902ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpacino30B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d558dd-6b9a-442b-8886-c3228f7cb3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fa0e4acd43455fb46f16bb5e200998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3145b144074f08b1c1de8b8d4fee7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating pipeline\n",
      "Instantiating HuggingFacePipeline\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digitous/Alpacino30b\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"digitous/Alpacino30b\")\n",
    "\n",
    "print('Instantiating pipeline')\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2)\n",
    "\n",
    "print('Instantiating HuggingFacePipeline')\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2ca003-120c-47da-955a-ef8474b03572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who will win the bundesliga this year?\n",
      "Hmmm.... Bayern obviously but also Dortmund and Leverkusen.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who will win the bundesliga this year?\"\n",
    "print(prompt + local_llm(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13854453-2b1d-4bdf-80f0-0617f627f6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMS Environment",
   "language": "python",
   "name": "llms_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
